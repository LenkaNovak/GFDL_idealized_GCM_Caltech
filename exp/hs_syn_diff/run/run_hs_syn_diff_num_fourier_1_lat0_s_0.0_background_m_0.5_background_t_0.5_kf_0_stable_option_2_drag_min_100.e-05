#!/bin/csh -f  
#SBATCH --ntasks=1
#SBATCH --time=20:00:00 
#SBATCH --job-name=fms_default
#SBATCH --output=out_err/slurm_%j.out
#SBATCH --error=out_err/slurm_%j.err 
#SBATCH --export=ALL
#SBATCH --partition=any

# === Default test run script for idealized GCM ===

# See description at run_test.readme
 
# Ian Eisenman, Yohai Kaspi, Tim Merlis, November 2010
# Farid Ait Chaalal, Xavier Levine, Zhihong Tan, March 2012 
# Farid Ait Chaalal, September 2012
# Robb Wills, Ori Adam, May 2014
# Lenka Novak, Oct 2018 

cd $SLURM_SUBMIT_DIR
echo $SLURM_SUBMIT_DIR 
limit stacksize unlimited 

# source required modules (HPC 2018) 
source /etc/profile.d/modules.csh

#--------------------------------------------------------------------------------------------------------
# Name definitions
#--------------------------------------------------------------------------------------------------------

# this run and its script
set run_name         = hs_syn_diff_num_fourier_1_lat0_s_0.0_background_m_0.5_background_t_0.5_kf_0_stable_option_2_drag_min_100.e-05                      # label for run; output dir and working dir are run_name specific (replaced if using sub/run_exp.sh)
set run_script       = $cwd/run_exp                  # path/name of this run script (for resubmit) 
set days             = 360                            # length of integration (e.g. # days)
set runs_per_script  = 5                             # number of runs within this script (e.g. # years)
set init_cond        = ""                            # overwritten later if reload_commands exists

# model type 
set model_type       = dry                           # if "moist", the moist model is run and if "dry, it is the dry. "moist_hydro" is for the bucket hydrology model. The namelists for the parameters are below (L212). 

# experiment directories
set exp_home       = $cwd:h                         # directory containing run/$run_script and input/ 
set exp_name       = $exp_home:t                    # name of experiment (i.e., name of this model build) 
set fms_home       = $cwd:h:h:h/idealized           # directory containing model source code, etc, usually /home/$USER/fms/idealized

set work_dir     = /central/scratch/elencz/caltech_fms_idealized/$exp_name/$run_name/workdir                  # where model is run and model output is produced
set output_dir   = /central/scratch/elencz/caltech_fms_idealized/$exp_name/$run_name/output                   # output directory will be created here
set reload_file = ${work_dir}/reload_commands 

# model source and namelists
set sourcedir   = $fms_home/src                                    # path to directory containing model source code
set namelist    = $exp_home/input/namelists_${model_type}          # path to namelist file
set fieldtable  = $exp_home/input/field_table_${model_type}        # path to field table (specifies tracers)
set time_stamp  = $fms_home/bin/time_stamp.csh                     # generates string date for file name labels

# executables 
set platform     = ifc_hpc                                # a unique identifier for your platform
set pathnames    = $exp_home/input/path_names                       # path to file containing list of source paths
set execdir      = $work_dir/exe.fms                                  # where code is compiled and executable is created
set mppnccombine = $work_dir/mppnccombine.ifc                   # path to executable mppnccombine
set template     = $fms_home/bin/mkmf.template.${platform}_mpi  # path to template for your platform
set mkmf         = $fms_home/bin/mkmf                               # path to executable mkmf

# resubmission 
set num_script_runs  = 1                             # how many times to resubmit script to queue
set ireload          = 1                             # counter for resubmitting this run script
set irun             = 1                             # counter for multiple model submissions within this script

# setup diagnostics analysis scripts and parameters
set analysis_type    = 3d                             # choose type of analysis: 2d (zonally averaged) or 3d (zonally varying) outputs
set start_analysis   = 5                              # number of script runs at which to start analysis (e.g. years after spin-up) 
set days_per_segment = 1                              # days per segment of analysis (for seasonally-varying analysis)
@ num_segments       = ${days} / ${days_per_segment}  # number of analysis segments
echo num_segments    = $num_segments
set out_freq         = '1xday'                        #also needs to be changed in ../input/diag_table 

set analysis_version = analysis_${analysis_type}                                    # type of analysis 
set analysis_dir     = ${fms_home:h}/analysis/$analysis_version/run                 # location of analysis directory
set analysis_script  = run_analysis_${model_type}_${analysis_type}_hpc              # analysis script 

set diagtable        = $exp_home/input/diag_table_${model_type}_${analysis_type}    # lists diagnostic variables

set run_analysis     = $work_dir/analysis                                           # where analysis is run
set analysis_out_err = $run_analysis/out_err                                        # out and err for analysis

# print config info
set echo  
echo "*** Running ${run_script} on $HOSTNAME ***"
echo "*** $analysis_version analysed in $num_segments segments ($days days / $days_per_segment days per segment) ***"
date

#--------------------------------------------------------------------------------------------------------
# Setup directory structure
#--------------------------------------------------------------------------------------------------------
cd $exp_home

if ( -d $work_dir ) then
  mv $work_dir work_dir_old
  echo "WARNING: Existing work_dir $work_dir removed."
  rm -rf work_dir_old
endif

mkdir $work_dir
mkdir $work_dir/INPUT 
mkdir $work_dir/RESTART

# if exists, load reload file
if ( -d $work_dir )  then
  if ( -f $reload_file ) then
     # set irun, ireload, init_cond
     source $reload_file
  endif
endif

if ( ! -d $cwd/run/out_err ) mkdir -p $cwd/run/out_err
if ( ! -d $execdir ) mkdir -p $execdir
if ( ! -d $run_analysis ) mkdir -p $run_analysis
if ( ! -d $analysis_out_err ) mkdir -p $analysis_out_err

if ( ! -d $output_dir )  then
  mkdir -p $output_dir
  mkdir -p $output_dir/combine  
  mkdir -p $output_dir/logfiles
  mkdir -p $output_dir/restart
endif

#--------------------------------------------------------------------------------------------------------
# Compile mppnccombine.c, needed only if $npes > 1
#--------------------------------------------------------------------------------------------------------

if ( ! -f $mppnccombine ) then
  gcc -O -o $mppnccombine -I$fms_home/bin/nc_inc -L$fms_home/bin/nc_lib $fms_home/postprocessing/mppnccombine.c -lnetcdf
endif

#--------------------------------------------------------------------------------------------------------
# Compile the model code and create executable
#--------------------------------------------------------------------------------------------------------

# append fms_home (containing netcdf libraries and include files) to template
/bin/cp $template $work_dir/tmp_template
echo "fms_home = $fms_home" >> $work_dir/tmp_template

# Prepend fortran files in srcmods directory to pathnames. 
# Use 'find' to make list of srcmod/*.f90 files. mkmf uses only the first instance of any file name.
cd $sourcedir
find $exp_home/srcmods/ -maxdepth 1 -iname "*.f90" -o -iname "*.inc" -o -iname "*.c" -o -iname "*.h" > $work_dir/tmp_pathnames
echo "Using the following sourcecode modifications:"
cat $work_dir/tmp_pathnames
cat $pathnames >> $work_dir/tmp_pathnames

cd $execdir
$mkmf -p fms.x -t $work_dir/tmp_template -c "-Duse_libMPI -Duse_netCDF" -a $sourcedir $work_dir/tmp_pathnames $sourcedir/shared/include $sourcedir/shared/mpp/include
make -f Makefile

cd $work_dir/INPUT

#--------------------------------------------------------------------------------------------------------
# set initial conditions and move to executable directory
#--------------------------------------------------------------------------------------------------------

if ( $init_cond != "" ) then
  cp $init_cond $init_cond:t
  cpio -iv  < $init_cond:t
#  rm -f $init_cond:t
endif

# name of ocean mask file, will only be used if load_mask = .true. in atmosphere_nml
set ocean_mask = ocean_mask_T42.nc

# if ocean_mask exists, move it to work_dir/INPUT folder O.A. May 2014
if (-e $exp_home/input/${ocean_mask}) then
   cp $exp_home/input/${ocean_mask} ocean_mask.nc
   cd $output_dir
   cp $exp_home/input/${ocean_mask} ocean_mask.nc
endif

#--------------------------------------------------------------------------------------------------------
#  --- Loop over $irun ---                                     
#--------------------------------------------------------------------------------------------------------


while ($irun <= $runs_per_script)

    cd $work_dir  

    # set run length and time step, get input data and executable
    if ( $ireload == 1 && $irun == 1 ) then
      cat > input.nml <<EOF
	&main_nml
        current_time = 0,
        override = .true.,
        days   = $days,
        dt_atmos = 300 /
EOF
    else
      cat > input.nml <<EOF
	&main_nml
        days   = $days,
        dt_atmos = 300 /
EOF
    endif

    if (${model_type} == dry) then 
      cat >> input.nml <<EOF

      &atmosphere_nml      
        two_stream           = .false.,
        turb                 = .true.,
        ldry_convection      = .false.,
        dry_model            = .true.,
        lwet_convection      = .false.,
        mixed_layer_bc       = .false.,
        do_virtual           = .false.,
        tapio_forcing        = .false.,
        hs                   = .true.,
        atmos_water_correction = .false.,
        roughness_mom        = 0.05, 
        roughness_heat       = 0.05,
        roughness_moist      = 0.05,
        bucket               = .false./

EOF
    else if (${model_type} == moist) then

      cat >> input.nml <<EOF    
  
      &atmosphere_nml
        two_stream           = .true.,
        turb                 = .true.,
        ldry_convection      = .false., 
        dry_model            = .false.,
        lwet_convection      = .true.,
        mixed_layer_bc       = .true.,
        do_virtual           = .true.,
        tapio_forcing        = .false., 
        hs                   = .false.,
        atmos_water_correction = .false.,
        roughness_mom        = 5e-03,
        roughness_heat       = 1e-05,
        roughness_moist      = 1e-05,
        bucket               = .false./


EOF

    else if (${model_type} == moist_hydro) then

      cat >> input.nml <<EOF    
  
      &atmosphere_nml
        two_stream           = .true.,
        turb                 = .true.,
        ldry_convection      = .false., 
        dry_model            = .false.,
        lwet_convection      = .true.,
        mixed_layer_bc       = .true.,
        do_virtual           = .true.,
        tapio_forcing        = .false., 
        hs                   = .false.,
        atmos_water_correction = .false.,
        roughness_mom        = 5e-03,
        roughness_heat       = 1e-05,
        roughness_moist      = 1e-05,
        bucket               = .true., 
        load_mask            = .true.,
        init_bucket_depth    = 1000., 
        init_bucket_depth_land = 1., 
        land_left            = 0.,
        land_right           = 360.,
        land_bottom          = 10.,
        land_top             = 30.,
        max_bucket_depth_land= 2., 
        robert_bucket        = 0.04,   
        raw_bucket           = 0.53,       
        damping_coeff_bucket = 200/

EOF
   endif

    cat >> input.nml <<EOF


      &grid_phys_list 
        tsfc_sp                  = 260.0, 
        delh                     = 90.,
        ka_days                  = 50.0,
        ks_days                  = 7.0,
        Cdrag                    = 0.0e-5,
        t_strat                  = 200.0,
        sigma_b                  = 0.85,
        scale_height_ratio       = 3.5,
        reference_sea_level_press = 100000.,
        phi0                     = 0.0/

      &dry_convection_nml
        gamma                    = 0.7,
        tau                      = 14400.0/

      &spectral_init_cond_nml
	      initial_temperature  = 280.0 /

      &diag_manager_nml
        mix_snapshot_average_fields = .true. /

      &radiation_nml
        albedo_value                 = 0.38,  
        lw_linear_frac               = 0.2,
        perpetual_equinox            =.false.,
        annual_mean                  =.false.,
        fixed_day                    =.false.,
        fixed_day_value              = 90.0,
        solar_constant               = 1360,
        lw_tau_exponent              = 4.0, 
        sw_tau_exponent              = 2.0,
        lw_tau0_pole                 = 1.8,
        lw_tau0_eqtr                 = 7.2,
        del_sol                      = 1.2,
        atm_abs                      = 0.22,
        days_in_year                 = 360,
        orb_long_perh                = 0,
        orb_obl                      = 23.5,
        orb_ecc                      = 0.0 /

      
      &mixed_layer_nml
        depth              = 10.0,
        qflux_amp          = 10.0,
        qflux_width        = 16.0,
        ekman_layer        = .false.,
        load_qflux         = .false.,
        evaporation        = .true.,
	      depth_land         = 1.0/


      &qe_moist_convection_nml
        tau_bm               = 7200.0,
        rhbm                 = 0.7,
        val_inc              = 0.01,
        Tmin                 = 50.,
        Tmax                 = 450. /

      &lscale_cond_nml
      	do_evap              = .false./

      # requires topography_option = 'gaussian' in spectral_dynamics_nml
      &gaussian_topog_nml
        height		 = 0.0,
        olon		   = 90.0,
        olat		   = 35.0,
        rlon       = 0.0,
        rlat       = 2.5,
        wlon		   = 4.95,
        wlat   		 = 4.95 /

      # LN experiments
      &spectral_dynamics_nml       
        damping_option          = 'resolution_dependent', 
        damping_order           = 4,
        damping_coeff           = 6.9444444e-05,
        cutoff_wn               = 15,
        do_mass_correction      =.true.,
        do_energy_correction    =.true.,
        do_water_correction     =.false.,
        do_spec_tracer_filter   =.false.,
        use_virtual_temperature =.false.,
        vert_advect_uv          = 'second_centered',
        vert_advect_t           = 'second_centered',
        longitude_origin        = 0.,
        robert_coeff            = .04,
        raw_factor              = 0.53,
        alpha_implicit          = .5, 
        reference_sea_level_press=1.e5, 
        lon_max                 = 128,
        lat_max                 = 64,
        num_levels              = 10,
num_fourier = 1,
        num_spherical           = 43,
        fourier_inc             = 1,
        triang_trunc            =.true.,
        valid_range_t 	    = 100. 800.,
        vert_coord_option       = 'uneven_sigma',
        topography_option       = 'flat',
        surf_res                = 0.1,
        scale_heights           = 5.0,
        exponent                = 2.0, 
        do_no_eddy_eddy         = .false. / 
      
      &hs_forcing_nml 
lat0_s = 0.0,
kf = 0,
        /  # remove rayleigh damping

      &diffusivity_nml
        pbl_mcm                 = .true.,
        free_atm_diff           = .false.,
        entr_ratio              = 0.0,
        mix_len	           = 30.0,
        fixed_depth             = .true.,	
        depth_0                 = 2500.0,
background_m = 0.5,
background_t = 0.5,
        frac_inner              = 0.015,
        diff_sth_fac            = 1.0,
        /

      &monin_obukhov_nml
        neutral                 = .false.,
        rich_crit               = 2.0,
drag_min = 100.e-05,
stable_option = 2,
        /

EOF
    endif

    cat $namelist >> input.nml
    cp $diagtable diag_table
    cp $fieldtable field_table
    cp $execdir/fms.x fms.x

    cp input.nml $work_dir

    #   --- run the model with mpirun  --- 

    set MX_RCACHE=2
    srun ${work_dir}/fms.x
    
    #   --- generate date for file names ---

    set date_name = `$time_stamp -eh`
    if ( $date_name == "" ) set date_name = tmp`date '+%j%H%M%S'`
    if ( -f time_stamp.out ) rm -f time_stamp.out

    #   --- move output files to their own directories (don't combine) --- 

    mkdir $output_dir/combine/$date_name

    foreach ncfile ( `/bin/ls *.nc *.nc.????` )
	    mv $ncfile $output_dir/combine/$date_name/$date_name.$ncfile
    end

    #   --- save ascii output files to local disk ---

    foreach out (`/bin/ls *.out`)
	    mv $out $output_dir/logfiles/$date_name.$out
    end

    #   --- move restart files to output directory --- 

    cd $work_dir/RESTART
    set resfiles = `/bin/ls *.res*`
    if ( $#resfiles > 0 ) then
      #     --- desired filename for cpio of output restart files ---	
      set restart_file = $output_dir/restart/$date_name.cpio
      if ( ! -d $restart_file:h ) mkdir -p $restart_file:h
      #     --- also save namelist and diag_table ---
      cp $work_dir/{*.nml,diag_table} .
      set files = ( $resfiles input.nml diag_table )
      /bin/ls $files | cpio -ocv > $restart_file:t
      mv $restart_file:t $restart_file
      #     --- set up restart for next run ---
      if ( $irun < $runs_per_script ) then 
          mv -f *.res*  ../INPUT
      endif
    endif

    cd $work_dir

    #--------------------------------------------------------------------------------------------------------

    #   --- write new reload information ---
    # for comparison with $start_analysis,  run_number = (ireload-1)*runs_per_script + irun
    set run_number = `expr $ireload \* $runs_per_script - $runs_per_script + $irun`
    echo Completed run $irun of $runs_per_script in bsub $ireload.
    set irun_prev = $irun
    @ irun++

    # remove restart file (init_cond) that is no longer in {reload_file} or ${reload_file}_prev
    if ( -f $reload_file"_prev" ) then
        set irun_tmp = $irun
        set ireload_tmp = $ireload
        set init_cond_tmp = $init_cond
        source $reload_file"_prev"
        rm -r $init_cond
        set irun = $irun_tmp
        set ireload = $ireload_tmp
        set init_cond = $init_cond_tmp
    endif
    if ( -f $reload_file ) mv -f $reload_file $reload_file"_prev"
    
    if ( $irun <= $runs_per_script ) then
    	  echo "set irun         =  $irun"          >  $reload_file
    else
        @ ireload++
        echo "set irun         =  1"              >  $reload_file
    endif

    echo     "set init_cond    =  $restart_file"  >> $reload_file
    echo     "set ireload      =  $ireload"       >> $reload_file
 
    ############################# post processing ############################
    
    cd $run_analysis
    if (${run_number} >= ${start_analysis}) then # combine data and do analysis

        # need to be careful not to write on top of file for analysis job currently pending in queue.
        # put each job in separate directory.
        set postproc_dir = ${run_analysis}/${date_name} # directory for this analysis run
        
        if ( ! -e $postproc_dir ) then
          mkdir -p ${postproc_dir}
        else
          rm -rf ${postproc_dir}
          mkdir ${postproc_dir}
          echo "WARNING: Existing analysis directory ${postproc_dir} removed."
        endif
        cd ${postproc_dir}
    
        echo "set exp_name = $exp_name" > post_processing_info 
        echo "set run_name = $run_name" >> post_processing_info
        echo "set date_name = $date_name" >> post_processing_info
        echo "set run_analysis = $run_analysis" >> post_processing_info
        echo "set output_dir = $output_dir" >> post_processing_info
        echo "set fms_home = $fms_home" >> post_processing_info
        echo "set work_dir = $work_dir" >> post_processing_info
        echo "set tmpdir = $work_dir" >> post_processing_info
              # specify model resolution, which is set in spectral_dynamics_nml section of input/namelists
              echo "set `grep um_fourier $work_dir/input.nml | tr ',' ' ' `" >> post_processing_info
        echo "set irun = $irun_prev" >> post_processing_info 
        echo "set runs_per_script = $runs_per_script" >> post_processing_info
              # information for segmentation of analysis
              echo "set days_per_segment = $days_per_segment" >> post_processing_info
        echo "set num_segments = $num_segments" >> post_processing_info
        echo "set isegment = 1" >> post_processing_info
        echo "set fms_output_freq = $out_freq" >> post_processing_info
    
        cp $analysis_dir/$analysis_script ./

        # ssh to head node and submit analysis script - once the double srun issue is sorted - run analysis script separately for now...
    else
	      #rm -rf $output_dir/combine/$date_name
    endif

    # don't resubmit if model failed to build a restart file
    if ( ! -f $restart_file ) then
      echo "FATAL ERROR: model restart file not saved. Try moving ${reload_file}_prev to ${reload_file} and re-running."
      echo "ireload = $ireload, irun = $irun"
      set irun = `expr $runs_per_script + 1 `
      set ireload = `expr $num_script_runs + 1 `
    endif


end # --- loop over $irun ended ---

#--------------------------------------------------------------------------------------------------------
#  --- Clean up ---                                     
#--------------------------------------------------------------------------------------------------------

cd $exp_home/run

if ($ireload > $num_script_runs) then
  echo "Note: not resubmitting job."
else
  echo "Submitting run $ireload."
  srun $run_script
endif

date

# CREATE THE SHELL TO RUN ANALYSIS RUN SRCIPTS (LN) 
cd $run_analysis
echo "#\!/bin/bash" >> run_all
echo ' ' >> run_all
echo 'module load intel/18.1' >> run_all
echo 'for d in day*; do' >> run_all
echo '    test -d "${d}" && cd "${d}" && sbatch run_analysis_* && cd "../"'  >> run_all
echo 'done' >> run_all

# run the analysis script
chmod 775 run_all
./run_all

#rm -rf $work_dir
