#!/bin/csh -f
#BSUB -n 1
#BSUB -W 12:00
#BSUB -J analysis3d
#BSUB -o ../out_err/out.analysis.%J
#BSUB -e ../out_err/err.analysis.%J
#BSUB -L /bin/csh 
#BSUB -R "rusage[scratch=320000]" 
#BSUB -R "rusage[mem=56000]"

#Note: 320000 MB scratch space and 56000 MB RAM should work for output of up to 90 days (T85, 180 days at T42)

# See description at run_analysis_moist_3d.readme

# Ian Eisenman, Yohai Kaspi, Tim Merlis, November 2010

# Xavier Levine, February 2012

# Farid Ait Chaalal, August 2012

# Robert Wills, September 2012

# Robert Wills, Zhihong Tan, August 2013

source /etc/profile.d/modules.csh
module load intel/12.1.2
module load open_mpi/1.4.5

limit stacksize unlimited
set echo

set NPROCS = `echo $LSB_HOSTS| wc -w`
echo This job has allocated $NPROCS cpus

#change the working directory (default is home directory)                   
cd $LS_SUBCWD
echo Working directory is $cwd

# maximum and minimum potential temperature values for isentropic analyses
set PotTmin           = 170
set PotTmax           = 650

# default frequency of FMS output [can be overwritten by post_processing_info] 
set fms_output_freq   = '4xday'
set fms_surface_freq  = '1x20days'

# whether to concatenate multiple time segments of analysis output (history) into a single netcdf file, only used if num_segments > 1 [can be overwritten by post_processing_info]
set concatenate_history = 0                 # (=1) single netcdf file with all times, (=0) separate files each with single time

# file generated by model run script to set exp_name, data_dir, run_name, date_name, run_analysis, fms_home, tmpdir, num_fourier, irun, runs_per_script, days_per_segment, num_segments, i_segment, final
source post_processing_info

@ times_per_segment = ${days_per_segment} * ` echo ${fms_output_freq} | cut -c 1 ` # number of instances in each segment
echo "$times_per_segment instants per segment, frequency $fms_output_freq."

# directories
set analysis_dir   = $fms_home:h/analysis/analysis_3d           # directory with analysis code
set run_dir        = ${tmpdir1}/${run_name}                  # tmp directory for current run
set scratch_dir    = $TMPDIR/${exp_name}/${run_name}     # scratch directory on specific compute node (faster read/write)
set uncombined_dir = ${tmpdir1}/${run_name}/output/combine/${date_name} # directory with uncombined input data
set input_dir      = ${scratch_dir}/combine                     # directory where combined netcdf files are written
set output_dir     = ${scratch_dir}/history                     # directory where output is written

set src_dir        = $analysis_dir/src                          # prefix for path_names
set exe_dir        = $run_analysis/exe.analysis                 # executable directory
set executable     = analysis                                   # executable name
set mppnccombine   = ${tmpdir1}/mppnccombine.ifc # path to the combine executable
set include_dir    = $fms_home/bin/nc_inc 
set template       = $analysis_dir/input/mkmf.template.ifc_brutus # machine-specific compilation templates for your platform
set diag_table     = $analysis_dir/input/diag_table.dt
set pathnames      = $analysis_dir/input/path_list              # file containing list of code

alias mkmf           $fms_home/bin/mkmf                         # produces makefile

echo "*** Running ${analysis_dir}/run_analysis_dry_3d for ${date_name} of ${run_name} on $HOSTNAME ***"

#### STEP 1: Combine the data files ####

mkdir -p ${input_dir}
cd ${scratch_dir}
foreach ncfile (`/bin/ls $uncombined_dir/${date_name}.*.nc.0000`)
  cp $ncfile:r.???? ${scratch_dir}
  set ncfile_tail = $ncfile:t
  rm -f $ncfile_tail:r
  $mppnccombine $ncfile_tail:r
  if ($status == 0) then
     mv $ncfile_tail:r ${input_dir}
     echo "$ncfile:r combined in ${scratch_dir} on $HOSTNAME"
  endif
  rm -f $ncfile_tail:r.????
end


#### STEP 2: Run the analysis ####

set successful_analysis = 1 # this gets set to 0 if an indicator suggests any instance of analysis failed.

# Build analysis code
if ( ! -d $exe_dir ) mkdir -p $exe_dir
if ( ! -d $output_dir )  mkdir $output_dir
cd $exe_dir
# create make file
# append fms_home (containing netcdf libraries and include files) to template
echo "fms_home =  $fms_home" > $exe_dir/tmp_template
/bin/cat $template >> $exe_dir/tmp_template
mkmf -a $src_dir -c"-Daix" -t $exe_dir/tmp_template -p $executable $pathnames $include_dir
make $executable
cp $executable $run_analysis/$date_name
cp $diag_table $run_analysis/$date_name/diag_table

cd $run_analysis/$date_name
# Loop over analysis segments and run analysis on each one
while ($isegment <= $num_segments)
  if ( $num_segments == 1) then
    set output_file_name = $output_dir/$date_name.nc
    set TimeIn = 0
  else
    set output_file_name = $output_dir/$date_name.segment${isegment}.nc
    @ TimeIn = ( ${isegment} - 1) * ${times_per_segment}
    echo 'Segment ' ${isegment}
    echo TimeIn = $TimeIn
  endif

  set filename_list = `cat -e` <<EOF
    &filename_list
      InputFileName = '$input_dir/$date_name.${fms_output_freq}.nc',
      OutputFileName = '$output_file_name'/
EOF
    
  echo $filename_list  | tr \$ "\n" >  $run_analysis/$date_name/input.nml

  set main_list = `cat -e` <<EOF
    &main_list
      DataIn = 1,
      data_source = 1,
      num_fourier = $num_fourier,
      TimeIn     = $TimeIn, 
      smooth_surface = .true.,
      MaxIsentrLev = 100,
      PotTempMin = $PotTmin,
      PotTempMax = $PotTmax,
      UVarName  =  'ucomp',
      VVarName  =  'vcomp',
      VorVarName = 'vor',
      DivVarName = 'div',
      TempVarName = 'temp',
      TSVarName = 't_surf',
      PSVarName = 'ps',
      ShumVarName = 'sphum',
      CondVarName = 'dt_qg_condensation',
      ConvVarName = 'dt_qg_convection',
      DiffVarName = 'dt_qg_diffusion', 
      PrecipCondVarName = 'condensation_rain',
      PrecipConvVarName = 'convection_rain',
      ToaFluxSWVarName = 'swdn_toa',
      ToaFluxLWUVarName = 'lwup_toa', 
      SfcFluxSWVarName = 'swdn_sfc',
      SfcFluxLWDVarName = 'lwdn_sfc',
      SfcFluxLWUVarName = 'lwup_sfc',
      SfcFluxLHVarName = 'flux_lhe',
      SfcFluxSHVarName = 'flux_t',
      SfcFluxQFVarName = 'flux_oceanq', 
      DiabCondVarName = 'dt_tg_condensation',
      DiabConvVarName = 'dt_tg_convection',
      DiabDiffVarName = 'dt_tg_diffusion',
      DiabRadVarName = 'tdt_rad',
      DiabSWVarName = 'tdt_solar', 
      BucketDepthVarName = 'bucket_depth', 
      BucketDepthConvVarName = 'bucket_depth_conv',
      BucketDepthCondVarName = 'bucket_depth_cond', 
      BucketDepthLHVarName = 'bucket_depth_lh',
      BucketDiffusionVarName = 'bucket_diffusion',
      DragMOVarName = 'drag_coeff_mo',
      DragLHVarName = 'drag_coeff_lh',
      DragSHVarName = 'drag_coeff_sh',
      is_gaussian = .true.,
      isentrope = .true., 
      eddy_fluxes = .true.,
      moisture  = .false.,
      virtual   = .false.,
      bucket    = .false.,
      precip_daily_threshold = 1.1574e-5,
      moist_isentropes = .false.,
      num_segments = ${num_segments},
      num_pbin = 41,
      num_bin = 41/
EOF

  echo $main_list  | tr \$ "\n" >>  $run_analysis/$date_name/input.nml

  ./$executable
  set analysis_return_value = $?
  echo 'Return value of analysis' $analysis_return_value
  if ( $analysis_return_value != 0 ) set successful_analysis = 0

  @ isegment++

end # loop over $isegment ended

if ( $successful_analysis == 1 ) then

  #### STEP 3: Move model output (analysis and surface) to data_dir ####

  # copy model history and logfiles to ${data_dir}
  mkdir -p ${data_dir}/history
  mkdir -p ${data_dir}/logfiles
  mkdir -p ${data_dir}/surface
  
  mv -f $output_dir/${date_name}*.nc  ${data_dir}/history/
  mv -f $input_dir/${date_name}.${fms_surface_freq}.nc ${data_dir}/surface/
  mv -f $uncombined_dir:h:h/logfiles/${date_name}.* ${data_dir}/logfiles/

  # copy run script and srcmods to a tar file in output (overwrite this file with each submission of this run)
  cd $fms_home:h/exp/${exp_name}/
  tar czvf ${data_dir}/scripts.tgz run/run_${run_name} srcmods/

endif

# remove combined input files from /scratch dir (leaving uncombined files in ~/fms_tmp/...)
rm -f $input_dir/${date_name}.*.nc
# also remove any output files (normally should not be any)
rm -f $output_dir/${date_name}.*.nc

#### STEP 4: Check if files are successfully in data_dir; if so, remove original uncombined files from model output directory ####

set successful_data = 1

# check time-mean history output for each segment (make sure file exists and is not too small)
cd ${data_dir}/history
set isegment          = 1
while ($isegment <= $num_segments)
  if ( $num_segments == 1) then
    set output_file_name = $date_name.nc
  else
    set output_file_name = $date_name.segment${isegment}.nc
  endif
  if ( -e $output_file_name) then
    set sz = `stat -c %s $output_file_name `
    if ( $sz < 1000000 ) set successful_data = 0
  else
    set successful_data = 0
  endif
  @ isegment++
end

# check surface output (make sure file exists - size depends on number of times)
#cd ${data_dir}/surface
#if ( ! -e ${date_name}.${fms_surface_freq}.nc ) set successful_data = 0

# if these tests passed, remove uncombined model output
if ( ${successful_data} == 1 ) then
  echo "  History and surface files at ${data_dir}, removing uncombined files at ${uncombined_dir}"
  rm -f ${uncombined_dir}/${date_name}.*.nc.????
  rmdir ${uncombined_dir}
endif

# clean up
rm $run_analysis/$date_name/$executable
rm $run_analysis/$date_name/diag_table
rm $run_analysis/$date_name/input.nml

